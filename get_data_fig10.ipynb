{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826395ad",
   "metadata": {},
   "source": [
    "# Get Figure 10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e1fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use for figure 10\n",
      "========================================\n",
      "GFDL-CM4\n",
      "GFDL-ESM4\n",
      "\n",
      "\n",
      "models to reject\n",
      "========================================\n",
      "models to use for figure 11\n",
      "========================================\n",
      "GFDL-CM4\n",
      "\n",
      "\n",
      "models to reject\n",
      "========================================\n",
      "ACCESS-CM2\n",
      "ACCESS-ESM1-5\n",
      "AWI-ESM-1-1-LR\n",
      "CESM2\n",
      "CESM2-FV2\n",
      "CMCC-CM2-SR5\n",
      "CMCC-ESM2\n",
      "CNRM-CM6-1\n",
      "CNRM-CM6-1-HR\n",
      "CNRM-ESM2-1\n",
      "CanESM5\n",
      "EC-Earth3\n",
      "EC-Earth3-AerChem\n",
      "EC-Earth3-LR\n",
      "HadGEM3-GC31-LL\n",
      "HadGEM3-GC31-MM\n",
      "IITM-ESM\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "IPSL-CM6A-LR\n",
      "MPI-ESM1-2-HR\n",
      "MPI-ESM1-2-LR\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "TaiESM1\n",
      "UKESM1-0-LL\n"
     ]
    }
   ],
   "source": [
    "# run the model validation notebook, which creates a variable `fig10_models`,\n",
    "# which is a list of models with the necessary fields\n",
    "experiment_id = 'piControl'\n",
    "%run find_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe7fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain we wish to study\n",
    "\n",
    "# test domain \n",
    "##################################################################\n",
    "#lats = (15, 20) # lat min, lat max\n",
    "#lons = (25, 29) # lon min, lon max\n",
    "#years = (2013, 2015) # start year, end year\n",
    "##################################################################\n",
    "\n",
    "# Thompson, MB\n",
    "##################################################################\n",
    "lats = (51, 57) # lat min, lat max\n",
    "lons = (259, 265) # lon min, lon max\n",
    "years = (1960, 2015) # start year, end year (note, no leap days)\n",
    "years = (400, 500)\n",
    "##################################################################\n",
    "\n",
    "save_data = True # save as netcdf for further processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d24580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problem models\n",
    "#fig10_models.remove('MIROC-ES2L')\n",
    "#fig10_models.remove('MIROC6')\n",
    "#fig10_models.remove('SAM0-UNICON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b45a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching domain:\n",
      "          source_id = 'GFDL-CM4'\n",
      "          experiment_id = 'piControl'\n",
      "          lats = (51, 57)\n",
      "          lons = (259, 265)\n",
      "          years = (400, 500)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "scrubbing NaN values\n",
      "merging datasets\n",
      "saving GFDL-CM4 to disk as netcdf\n",
      "success\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "          source_id = 'GFDL-ESM4'\n",
      "          experiment_id = 'piControl'\n",
      "          lats = (51, 57)\n",
      "          lons = (259, 265)\n",
      "          years = (400, 500)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "scrubbing NaN values\n",
      "merging datasets\n",
      "saving GFDL-ESM4 to disk as netcdf\n",
      "success\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parse each model in the list \n",
    "for source in fig10_models:\n",
    "    source_id = source\n",
    "    \n",
    "    # get all the 3hr fields\n",
    "    table_id = '3hr'\n",
    "    %run CMIP6_lib.ipynb\n",
    "    required_fields = ['tas', 'huss']\n",
    "    print(f\"\"\"Fetching domain:\n",
    "          {source_id = }\n",
    "          {experiment_id = }\n",
    "          {lats = }\n",
    "          {lons = }\n",
    "          {years = }\"\"\")\n",
    "    print(\"acquiring 3hrly data\")\n",
    "    # grab all fields of interest and combine (3hr)\n",
    "    my_fields = [get_field(field, df_in) for field in required_fields]\n",
    "    small_fields = [trim_field(field, lats, lons, years) for field in my_fields]\n",
    "    ds_3h = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "    \n",
    "    # filter extraneous dimensions\n",
    "    for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\"]:\n",
    "        try:\n",
    "            ds_3h = ds_3h.drop(dim)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # take spatial average\n",
    "    #ds_3h = ds_3h.mean(dim=(\"lat\", \"lon\"))\n",
    "\n",
    "    print(\"acquiring daily data\")\n",
    "    # get all the daily fields\n",
    "    table_id = 'day'\n",
    "    %run CMIP6_lib.ipynb\n",
    "    required_fields = ['mrsos']\n",
    "    \n",
    "    # grab all fields of interest and combine (day)\n",
    "    my_fields = [get_field(field, df_in) for field in required_fields]\n",
    "    small_fields = [trim_field(field, lats, lons, years) for field in my_fields]\n",
    "    ds_day = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "    \n",
    "    # filter extraneous dimensions\n",
    "    for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\", \"lat_bnds\", \"lon_bnds\"]:\n",
    "        try:\n",
    "            ds_day = ds_day.drop(dim)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # take spatial average\n",
    "    #ds_day = ds_day.mean(dim=(\"lat\", \"lon\"))\n",
    "    \n",
    "    # interpolate daily data onto the 3hourly and merge.\n",
    "    print(\"interpolating\")\n",
    "    day_interp = ds_day.interp_like(ds_3h).chunk({\"time\":-1})\n",
    "    print(\"scrubbing NaN values\")\n",
    "    day_interp = day_interp.interpolate_na(dim=\"time\")\n",
    "    print(\"merging datasets\")\n",
    "    full_dataset = ds_3h.merge(day_interp).metpy.quantify().chunk({\"time\":10000})\n",
    "    \n",
    "    \n",
    "    if save_data:\n",
    "        print(f\"saving {source_id} to disk as netcdf\")\n",
    "        full_dataset.to_netcdf(f\"./data/{source_id}-{experiment_id}-fig10.nc\", engine=\"netcdf4\")\n",
    "        \n",
    "        print(\"success\\n\\n\")\n",
    "    else:\n",
    "        print(f\"successfully parsed {source_id}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
