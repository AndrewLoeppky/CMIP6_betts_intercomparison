{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a797b59e",
   "metadata": {},
   "source": [
    "# CMIP Scraping Library\n",
    "\n",
    "Functions for downloading specific fields from CMIP6 model outputs. Turn this into a real library or module later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e457787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pooch\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "\n",
    "\n",
    "# Handy metpy tutorial working with xarray:\n",
    "# https://unidata.github.io/MetPy/latest/tutorials/xarray_tutorial.html#sphx-glr-tutorials-xarray-tutorial-py\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.units import units\n",
    "from metpy.plots import SkewT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3644d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_var_exact(the_dict,df_og):\n",
    "    \"\"\"\n",
    "    Jamie's code -- fetches variables with precicely defined coordinates \n",
    "    \n",
    "        source_id\n",
    "        table_id\n",
    "        variable_id\n",
    "        \n",
    "    Returns Xarray containing requested variable\n",
    "    \"\"\"\n",
    "    the_keys = list(the_dict.keys())\n",
    "    #print(the_keys)\n",
    "    key0 = the_keys[0]\n",
    "    #print(key0)\n",
    "    #print(the_dict[key0])\n",
    "    hit0 = df_og[key0] == the_dict[key0]\n",
    "    if len(the_keys) > 1:\n",
    "        hitnew = hit0\n",
    "        for key in the_keys[1:]:\n",
    "            hit = df_og[key] == the_dict[key]\n",
    "            hitnew = np.logical_and(hitnew,hit)\n",
    "            #print(\"total hits: \",np.sum(hitnew))\n",
    "    else:\n",
    "        hitnew = hit0\n",
    "    df_result = df_og[hitnew]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb59819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(variable_id, \n",
    "              df,\n",
    "              source_id,\n",
    "              experiment_id,\n",
    "              table_id):\n",
    "    \"\"\"\n",
    "    extracts a single variable field from the model\n",
    "    \"\"\"\n",
    "\n",
    "    var_dict = dict(source_id = source_id, variable_id = variable_id,\n",
    "                    experiment_id = experiment_id, table_id = table_id)\n",
    "    \n",
    "    local_var = fetch_var_exact(var_dict, df)\n",
    "    zstore_url = local_var['zstore'].array[0]\n",
    "    the_mapper=fsspec.get_mapper(zstore_url)\n",
    "    local_var = xr.open_zarr(the_mapper, consolidated=True)\n",
    "    return local_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26628a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_field(df, lat, lon, years):\n",
    "    \"\"\"\n",
    "    cuts out a specified domain from an xarrray field\n",
    "    \n",
    "    lat = (minlat, maxlat)\n",
    "    lon = (minlon, maxlon)\n",
    "    \"\"\"\n",
    "    new_field = df.sel(lat=slice(lat[0],lat[1]), lon=slice(lon[0],lon[1]))\n",
    "    \n",
    "    # handle cftime.DatetimeNoLeap time formatting\n",
    "    if new_field.time.dtype == 'O':\n",
    "        new_field = new_field.isel(time=(new_field.time.dt.year >= years[0]))\n",
    "        #new_field = new_field.isel(time=(new_field.time.dt.year <= years[1]))\n",
    "    \n",
    "    # handle numpy.datetime64[ns] time formatting\n",
    "    elif new_field.time.dtype == '<M8[ns]':\n",
    "        new_field = new_field.isel(time=(pd.to_datetime(df.time).year >= years[0]))\n",
    "        #new_field = new_field.isel(time=(pd.to_datetime(df.time).year < years[1]))\n",
    "        \n",
    "    return new_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ad7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_models(experiment_id, variables):\n",
    "    \"\"\"\n",
    "    Returns a list of models which contain the requested variables at requested intervals\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_id : string\n",
    "        model run selected from the list found here:\n",
    "            https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit#    \n",
    "    variables : dict\n",
    "        key value pairs {\"interval\":[\"var1\", \"var2\"]} specifying requested variables at each time interval\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    models : list\n",
    "        the source id's for CMIP6 models which fit the requested criteria. Also prints eligible models\n",
    "        to screen.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    In []:   find_models(\"piControl\", {\"3hr\":['tas', 'huss'], \"day\":['mrsos']})\n",
    "    \n",
    "    Out []:  [GFDL-CM4, GFDL-ESM4]\n",
    "    \"\"\"\n",
    "    # get all the data from google's datastore\n",
    "    odie = pooch.create(\n",
    "        path=\"./.cache\",\n",
    "        base_url=\"https://storage.googleapis.com/cmip6/\",\n",
    "        registry={\n",
    "            \"pangeo-cmip6.csv\": None\n",
    "        },\n",
    "    )\n",
    "    file_path = odie.fetch(\"pangeo-cmip6.csv\")\n",
    "    df_in = pd.read_csv(file_path)\n",
    "    \n",
    "    # select by experiment_id\n",
    "    df_expt = df_in[(df_in.experiment_id == experiment_id)]\n",
    "    \n",
    "    # create a dictionary containing all models that contain req'd variables\n",
    "    # for each interval\n",
    "    models = {}\n",
    "    for key in variables.keys():\n",
    "        df_var = df_expt[df_expt.table_id == key]\n",
    "        df_mods = df_var.groupby(\"source_id\")\n",
    "        models[key] = list(df_mods.groups.keys())\n",
    "        \n",
    "    # return models that have ALL req'd variables\n",
    "    models_pass = list(reduce(lambda i, j: i & j, (set(x) for x in list(models.values()))))\n",
    "    return models_pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73e8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(experiment_id, variables, domain, path, download=True):\n",
    "    \"\"\"\n",
    "    Finds model runs that fit specified criteria, selects a domain and saves fields to disk as NETCDF files. \n",
    "    ** NOTE: Current version only supports datasets with \"3hr\" and \"day\" intervals **\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_id : string\n",
    "        model run selected from the list found here:\n",
    "            https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit#    \n",
    "    variables : dict\n",
    "        key value pairs {\"interval\":[\"var1\", \"var2\"]} specifying requested variables at each time interval\n",
    "    domain : dict \n",
    "        specify lattitude and longitude bounds and time interval. Example for Thompson, MB, Canada from year 1960 to 2016:\n",
    "        \n",
    "        {\"lats\":(51, 57) \n",
    "         \"lons\":(259, 265) \n",
    "         \"years\":(1960, 2015)}\n",
    "        \n",
    "        NOTE: time formatting is inconsistent between models, i.e. some use hours since 01-01-1850 00:00:00, others begin\n",
    "              1960, others begin at arbitrary year zero for spin up...\n",
    "    path : str\n",
    "        location to save netcdf files. e.g. \"../CMIP_data\"\n",
    "    download : bool\n",
    "        set download=False for the first run to ensure all fields are present/correct. Saving to NETCDF can \n",
    "        take a long time, especially for large datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None. Prints a message to screen indicating success/failure of each model download\n",
    "    \"\"\"\n",
    "    # get all the data from google's datastore\n",
    "    odie = pooch.create(\n",
    "        path=\"./.cache\",\n",
    "        base_url=\"https://storage.googleapis.com/cmip6/\",\n",
    "        registry={\n",
    "            \"pangeo-cmip6.csv\": None\n",
    "        },\n",
    "    )\n",
    "    file_path = odie.fetch(\"pangeo-cmip6.csv\")\n",
    "    df_in = pd.read_csv(file_path)\n",
    "    \n",
    "    # find which models have required fields at desired intervals\n",
    "    models_to_use = find_models(experiment_id, variables)\n",
    "    print(f\"found {len(models_to_use)} compatible model runs\\n\")\n",
    "    \n",
    "    for model in models_to_use:\n",
    "        # download files if no errors (set this to False later on if error occurs) \n",
    "        if download:\n",
    "            do_download = True\n",
    "        else:\n",
    "            do_download = False\n",
    "            \n",
    "        source_id = model\n",
    "        # get all the 3hr fields\n",
    "        table_id = '3hr'\n",
    "        required_fields = variables['3hr']\n",
    "        print(f\"\"\"Fetching domain:\n",
    "              source_id = {source_id}\n",
    "              experiment_id = {experiment_id}\n",
    "              lats = {domain[\"lats\"]}\n",
    "              lons = {domain[\"lons\"]}\n",
    "              years = {domain[\"years\"]}\"\"\")\n",
    "        \n",
    "        print(\"acquiring 3hrly data\")\n",
    "        # grab all fields of interest and combine (3hr)\n",
    "        try:\n",
    "            my_fields = [get_field(field, df_in, source_id, experiment_id, table_id) for field in variables['3hr']]\n",
    "            small_fields = [trim_field(field, domain[\"lats\"], domain[\"lons\"], domain[\"years\"]) for field in my_fields]\n",
    "            ds_3h = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "        except IndexError:\n",
    "            print(f\"ERROR: {model}, required '3hr' field(s) missing or empty\")\n",
    "            do_download = False\n",
    "\n",
    "        # filter extraneous dimensions\n",
    "        for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\", \"lat_bnds\", \"lon_bnds\"]:\n",
    "            try:\n",
    "                ds_3h = ds_3h.drop(dim)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        print(\"acquiring daily data\")\n",
    "        # get all the daily fields\n",
    "        table_id = 'day'\n",
    "        required_fields = variables['day']\n",
    "\n",
    "        # grab all fields of interest and combine (day)\n",
    "        try:\n",
    "            my_fields = [get_field(field, df_in, source_id, experiment_id, table_id) for field in variables['day']]\n",
    "            small_fields = [trim_field(field, domain[\"lats\"], domain[\"lons\"], domain[\"years\"]) for field in my_fields]\n",
    "            ds_day = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "        except IndexError:\n",
    "            print(f\"ERROR: {model}, required 'day' field(s) missing or empty\")\n",
    "            do_download = False\n",
    "        except KeyError:\n",
    "            print(f\"ERROR: {model}, coordinate system not supported\")\n",
    "            do_download = False\n",
    "        \n",
    "        # filter extraneous dimensions\n",
    "        for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\", \"lat_bnds\", \"lon_bnds\"]:\n",
    "            try:\n",
    "                ds_day = ds_day.drop(dim)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # interpolate daily data onto the 3hourly and merge.\n",
    "        print(\"interpolating\")\n",
    "        try:\n",
    "            day_interp = ds_day.interp_like(ds_3h).chunk({\"time\":-1})\n",
    "            print(\"merging datasets\")\n",
    "            full_dataset = ds_3h.merge(day_interp, compat='override').metpy.quantify().chunk({\"time\":10000})\n",
    "        except TypeError:\n",
    "            print(f\"ERROR: {model}, unsupported datetime formatting\")\n",
    "            do_download = False\n",
    "            \n",
    "        print(\"scrubbing NaN values\")\n",
    "        # day_interp = day_interp.interpolate_na(dim=\"time\") # this needs to be fixed. working on\n",
    "        # 2022-04-10, pandas changed function since then???\n",
    "        \n",
    "        if do_download:\n",
    "            print(f\"saving {source_id} to disk as netcdf\")\n",
    "            full_dataset.to_netcdf(f\"{path}/{source_id}-{experiment_id}-fig10.nc\", engine=\"netcdf4\")\n",
    "            print(\"success\\n\\n\")\n",
    "        else:\n",
    "            print(f\"parsed {source_id}\\n\\n\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d984292",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_domain = {\"lats\":(51, 57), \"lons\":(259, 265), \"years\":(1960, 2015)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2cdf8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 34 compatible model runs\n",
      "Fetching domain:\n",
      "              source_id = CNRM-CM6-1\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CNRM-CM6-1\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = EC-Earth3-Veg\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "ERROR: EC-Earth3-Veg, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed EC-Earth3-Veg\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = AWI-ESM-1-1-LR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed AWI-ESM-1-1-LR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = CNRM-ESM2-1\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CNRM-ESM2-1\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = CMCC-ESM2\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CMCC-ESM2\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = UKESM1-0-LL\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: UKESM1-0-LL, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed UKESM1-0-LL\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = EC-Earth3\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed EC-Earth3\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = ACCESS-CM2\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed ACCESS-CM2\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = EC-Earth3-Veg-LR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed EC-Earth3-Veg-LR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = GFDL-CM4\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed GFDL-CM4\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = EC-Earth3-AerChem\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed EC-Earth3-AerChem\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MPI-ESM1-2-HR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MPI-ESM1-2-HR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MRI-ESM2-0\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MRI-ESM2-0\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = SAM0-UNICON\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed SAM0-UNICON\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = GISS-E2-1-G\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "ERROR: GISS-E2-1-G, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed GISS-E2-1-G\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = IPSL-CM6A-LR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: IPSL-CM6A-LR, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed IPSL-CM6A-LR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MPI-ESM1-2-LR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MPI-ESM1-2-LR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = KACE-1-0-G\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed KACE-1-0-G\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = AWI-CM-1-1-MR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: AWI-CM-1-1-MR, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "ERROR: AWI-CM-1-1-MR, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed AWI-CM-1-1-MR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = NESM3\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: NESM3, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "ERROR: NESM3, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed NESM3\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = TaiESM1\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "ERROR: TaiESM1, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed TaiESM1\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = CMCC-CM2-SR5\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CMCC-CM2-SR5\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = BCC-CSM2-MR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed BCC-CSM2-MR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MIROC6\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MIROC6\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = GFDL-ESM4\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "ERROR: GFDL-ESM4, required 'day' field(s) missing or empty\n",
      "interpolating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed GFDL-ESM4\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MIROC-ES2L\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MIROC-ES2L\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = HadGEM3-GC31-LL\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed HadGEM3-GC31-LL\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = CNRM-CM6-1-HR\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CNRM-CM6-1-HR\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = IPSL-CM5A2-INCA\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: IPSL-CM5A2-INCA, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "ERROR: IPSL-CM5A2-INCA, coordinate system not supported\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed IPSL-CM5A2-INCA\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = KIOST-ESM\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "ERROR: KIOST-ESM, required '3hr' field(s) missing or empty\n",
      "acquiring daily data\n",
      "ERROR: KIOST-ESM, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed KIOST-ESM\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = HadGEM3-GC31-MM\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "ERROR: HadGEM3-GC31-MM, required 'day' field(s) missing or empty\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed HadGEM3-GC31-MM\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = ACCESS-ESM1-5\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed ACCESS-ESM1-5\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = CanESM5\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed CanESM5\n",
      "\n",
      "\n",
      "Fetching domain:\n",
      "              source_id = MPI-ESM-1-2-HAM\n",
      "              experiment_id = historical\n",
      "              lats = (51, 57)\n",
      "              lons = (259, 265)\n",
      "              years = (1960, 2015)\n",
      "acquiring 3hrly data\n",
      "acquiring daily data\n",
      "interpolating\n",
      "merging datasets\n",
      "scrubbing NaN values\n",
      "parsed MPI-ESM-1-2-HAM\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_data(\"historical\", {\"3hr\":['tas', 'huss'], \"day\":['mrsos']}, my_domain, \"./data\", download=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
