{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a797b59e",
   "metadata": {},
   "source": [
    "# CMIP6 Library\n",
    "\n",
    "Contains functions for parsing and saving fields from the Climate Model Intercomparison Project (CMIP6), as well as plotting functions for recreating figures from Alan K Betts' 2009 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e457787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pooch\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "from itertools import cycle\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "# Handy metpy tutorial working with xarray:\n",
    "# https://unidata.github.io/MetPy/latest/tutorials/xarray_tutorial.html#sphx-glr-tutorials-xarray-tutorial-py\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.units import units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3644d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_var_exact(the_dict,df_og):\n",
    "    \"\"\"\n",
    "    Jamie's code -- fetches variables with precicely defined coordinates \n",
    "    \n",
    "        source_id\n",
    "        table_id\n",
    "        variable_id\n",
    "        \n",
    "    Returns Xarray containing requested variable\n",
    "    \"\"\"\n",
    "    the_keys = list(the_dict.keys())\n",
    "    #print(the_keys)\n",
    "    key0 = the_keys[0]\n",
    "    #print(key0)\n",
    "    #print(the_dict[key0])\n",
    "    hit0 = df_og[key0] == the_dict[key0]\n",
    "    if len(the_keys) > 1:\n",
    "        hitnew = hit0\n",
    "        for key in the_keys[1:]:\n",
    "            hit = df_og[key] == the_dict[key]\n",
    "            hitnew = np.logical_and(hitnew,hit)\n",
    "            #print(\"total hits: \",np.sum(hitnew))\n",
    "    else:\n",
    "        hitnew = hit0\n",
    "    df_result = df_og[hitnew]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb59819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(variable_id, \n",
    "              df,\n",
    "              source_id,\n",
    "              experiment_id,\n",
    "              table_id):\n",
    "    \"\"\"\n",
    "    extracts a single variable field from the model\n",
    "    \"\"\"\n",
    "\n",
    "    var_dict = dict(source_id = source_id, variable_id = variable_id,\n",
    "                    experiment_id = experiment_id, table_id = table_id)\n",
    "    \n",
    "    local_var = fetch_var_exact(var_dict, df)\n",
    "    zstore_url = local_var['zstore'].array[0]\n",
    "    the_mapper=fsspec.get_mapper(zstore_url)\n",
    "    local_var = xr.open_zarr(the_mapper, consolidated=True)\n",
    "    return local_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26628a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_field(df, lat, lon, years):\n",
    "    \"\"\"\n",
    "    cuts out a specified domain from an xarrray field\n",
    "    \n",
    "    lat = (minlat, maxlat)\n",
    "    lon = (minlon, maxlon)\n",
    "    \"\"\"\n",
    "    new_field = df.sel(lat=slice(lat[0],lat[1]), lon=slice(lon[0],lon[1]))\n",
    "    \n",
    "    # handle cftime.DatetimeNoLeap time formatting\n",
    "    if new_field.time.dtype == 'O':\n",
    "        new_field = new_field.isel(time=(new_field.time.dt.year >= years[0]))\n",
    "        #new_field = new_field.isel(time=(new_field.time.dt.year <= years[1]))\n",
    "    \n",
    "    # handle numpy.datetime64[ns] time formatting\n",
    "    elif new_field.time.dtype == '<M8[ns]':\n",
    "        new_field = new_field.isel(time=(pd.to_datetime(df.time).year >= years[0]))\n",
    "        #new_field = new_field.isel(time=(pd.to_datetime(df.time).year < years[1]))\n",
    "        \n",
    "    return new_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2655f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_models(experiment_id, variables):\n",
    "    \"\"\"\n",
    "    Returns a list of models which contain the requested variables at requested intervals\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_id : string\n",
    "        model run selected from the list found here:\n",
    "            https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit#    \n",
    "    variables : dict\n",
    "        key value pairs {\"interval\":[\"var1\", \"var2\"]} specifying requested variables at each time interval\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    models : list\n",
    "        the source id's for CMIP6 models which fit the requested criteria. Also prints eligible models\n",
    "        to screen.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    In []:   find_models(\"piControl\", {\"3hr\":['tas', 'huss'], \"day\":['mrsos']})\n",
    "    \n",
    "    Out []:  [GFDL-CM4, GFDL-ESM4]\n",
    "    \"\"\"\n",
    "    # get all the data from google's datastore\n",
    "    odie = pooch.create(\n",
    "        path=\"./.cache\",\n",
    "        base_url=\"https://storage.googleapis.com/cmip6/\",\n",
    "        registry={\n",
    "            \"pangeo-cmip6.csv\": None\n",
    "        },\n",
    "    )\n",
    "    file_path = odie.fetch(\"pangeo-cmip6.csv\")\n",
    "    df_in = pd.read_csv(file_path)\n",
    "    \n",
    "    # select by experiment_id\n",
    "    df_expt = df_in[(df_in.experiment_id == experiment_id)]\n",
    "    \n",
    "    # create a dictionary containing all models that contain req'd variables\n",
    "    # for each interval\n",
    "    models = {}\n",
    "    for key in variables.keys():\n",
    "        df_var = df_expt[df_expt.table_id == key]\n",
    "        df_mods = df_var.groupby(\"source_id\")\n",
    "        models[key] = list(df_mods.groups.keys())\n",
    "        \n",
    "    # return models that have ALL req'd variables\n",
    "    models_pass = list(reduce(lambda i, j: i & j, (set(x) for x in list(models.values()))))\n",
    "    return models_pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe42ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(experiment_id, variables, domain, path, download=True):\n",
    "    \"\"\"\n",
    "    Finds model runs that fit specified criteria, selects a domain and saves fields to disk as NETCDF files.\n",
    "    Silences exceptions but cancels downloads of buggy/incompatible models.\n",
    "    \n",
    "    ** NOTE: Current version only supports datasets with \"3hr\" and \"day\" intervals **\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_id : string\n",
    "        model run selected from the list found here:\n",
    "            https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit#    \n",
    "    variables : dict\n",
    "        key value pairs {\"interval\":[\"var1\", \"var2\"]} specifying requested variables at each time interval\n",
    "    domain : dict \n",
    "        specify lattitude and longitude bounds and time interval. Example for Thompson, MB, Canada from year 1960 to 2016:\n",
    "        \n",
    "        {\"lats\":(51, 57) \n",
    "         \"lons\":(259, 265) \n",
    "         \"years\":(1960, 2015)}\n",
    "        \n",
    "        NOTE: time formatting is inconsistent between models, i.e. some use hours since 01-01-1850 00:00:00, others begin\n",
    "              1960, others begin at arbitrary year zero for spin up...\n",
    "    path : str\n",
    "        location to save netcdf files. e.g. \"../CMIP_data\"\n",
    "    download : bool\n",
    "        set download=False for the first run to ensure all fields are present/correct. Saving to NETCDF can \n",
    "        take a long time, especially for large datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None. Prints a message to screen indicating success/failure of each model download\n",
    "    \"\"\"\n",
    "    # get all the data from google's datastore\n",
    "    odie = pooch.create(\n",
    "        path=\"./.cache\",\n",
    "        base_url=\"https://storage.googleapis.com/cmip6/\",\n",
    "        registry={\n",
    "            \"pangeo-cmip6.csv\": None\n",
    "        },\n",
    "    )\n",
    "    file_path = odie.fetch(\"pangeo-cmip6.csv\")\n",
    "    df_in = pd.read_csv(file_path)\n",
    "    \n",
    "    # find which models have required fields at desired intervals\n",
    "    models_to_use = find_models(experiment_id, variables)\n",
    "    print(f\"found {len(models_to_use)} compatible model runs\\n\")\n",
    "    \n",
    "    for model in models_to_use:\n",
    "        # download files if no errors (set this to False later on if error occurs) \n",
    "        if download:\n",
    "            do_download = True\n",
    "        else:\n",
    "            do_download = False\n",
    "            \n",
    "        source_id = model\n",
    "        # get all the 3hr fields\n",
    "        table_id = '3hr'\n",
    "        required_fields = variables['3hr']\n",
    "        print(f\"\"\"Fetching domain:\n",
    "              source_id = {source_id}\n",
    "              experiment_id = {experiment_id}\n",
    "              lats = {domain[\"lats\"]}\n",
    "              lons = {domain[\"lons\"]}\n",
    "              years = {domain[\"years\"]}\"\"\")\n",
    "        \n",
    "        print(\"acquiring 3hrly data\")\n",
    "        # grab all fields of interest and combine (3hr)\n",
    "        try:\n",
    "            my_fields = [get_field(field, df_in, source_id, experiment_id, table_id) for field in variables['3hr']]\n",
    "            small_fields = [trim_field(field, domain[\"lats\"], domain[\"lons\"], domain[\"years\"]) for field in my_fields]\n",
    "            ds_3h = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "        except IndexError:\n",
    "            print(f\"ERROR: {model}, required '3hr' field(s) missing or empty\")\n",
    "            do_download = False\n",
    "\n",
    "        # filter extraneous dimensions\n",
    "        for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\", \"lat_bnds\", \"lon_bnds\"]:\n",
    "            try:\n",
    "                ds_3h = ds_3h.drop(dim)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        print(\"acquiring daily data\")\n",
    "        # get all the daily fields\n",
    "        table_id = 'day'\n",
    "        required_fields = variables['day']\n",
    "\n",
    "        # grab all fields of interest and combine (day)\n",
    "        try:\n",
    "            my_fields = [get_field(field, df_in, source_id, experiment_id, table_id) for field in variables['day']]\n",
    "            small_fields = [trim_field(field, domain[\"lats\"], domain[\"lons\"], domain[\"years\"]) for field in my_fields]\n",
    "            ds_day = xr.combine_by_coords(small_fields, compat=\"override\", combine_attrs=\"drop_conflicts\")\n",
    "        except IndexError:\n",
    "            print(f\"ERROR: {model}, required 'day' field(s) missing or empty\")\n",
    "            do_download = False\n",
    "        except KeyError:\n",
    "            print(f\"ERROR: {model}, coordinate system not supported\")\n",
    "            do_download = False\n",
    "        \n",
    "        # filter extraneous dimensions\n",
    "        for dim in [\"height\", \"time_bounds\", \"depth\", \"depth_bounds\", \"lat_bnds\", \"lon_bnds\"]:\n",
    "            try:\n",
    "                ds_day = ds_day.drop(dim)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # interpolate daily data onto the 3hourly and merge.\n",
    "        print(\"interpolating\")\n",
    "        try:\n",
    "            day_interp = ds_day.interp_like(ds_3h).chunk({\"time\":-1})\n",
    "            print(\"merging datasets\")\n",
    "            full_dataset = ds_3h.merge(day_interp, compat='override').metpy.quantify().chunk({\"time\":10000})\n",
    "        except TypeError:\n",
    "            print(f\"ERROR: {model}, unsupported datetime formatting\")\n",
    "            do_download = False\n",
    "            \n",
    "        print(\"scrubbing NaN values\")\n",
    "        # day_interp = day_interp.interpolate_na(dim=\"time\") # this needs to be fixed. working on\n",
    "        # 2022-04-10, pandas changed function since then???\n",
    "        \n",
    "        if do_download:\n",
    "            print(f\"saving {source_id} to disk as netcdf\")\n",
    "            full_dataset.to_netcdf(f\"{path}/{source_id}-{experiment_id}.nc\", engine=\"netcdf4\")\n",
    "            print(\"success\\n\\n\")\n",
    "        else:\n",
    "            print(f\"parsed {source_id}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfcafcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_betts_fig10(nc_data, save_loc=None):\n",
    "    \"\"\"\n",
    "    Creates a plot matching figure 10 from 'Land-Surface-Atmosphere Coupling in Observations and Models', \n",
    "    by Alan K Betts (2009). Required fields are (interpolated to 3hr or better)\n",
    "    \n",
    "        3hr:  tas        (surface air temp)\n",
    "              huss       (surface humidity)\n",
    "              mrsos      (upper layer soil moisture)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_data : string\n",
    "        path do NETCDF data file saved using download_data() function in this library\n",
    "        \n",
    "    save_loc : string\n",
    "        location to save the figure. default behavior is to render the figure but not save. \n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    None. Saves plot to disk if save_loc is specified\n",
    "    \"\"\"\n",
    "    # open the data and re-convert time to cftime so xarray is happy\n",
    "    data_in = xr.open_dataset(nc_data, engine=\"netcdf4\", decode_times=False).metpy.quantify()\n",
    "    #data_in[\"time\"] = cftime.num2date(data_in.time, \"hours since 1850-01-01 00:00:00\", calendar=\"noleap\", has_year_zero=True)\n",
    "    data_in[\"time\"] = cftime.num2date(data_in.time, \"hours since 0001-01-01 03:00:00\", calendar=\"noleap\", has_year_zero=True)\n",
    "\n",
    "    # constant for surface pressure\n",
    "    ps = 100000 * units.Pa \n",
    "    \n",
    "    # use metpy to convert humidity field to dew point temp\n",
    "    try:\n",
    "        data_in[\"td\"] = mpcalc.dewpoint_from_specific_humidity(ps, data_in.tas, data_in.huss)\n",
    "    except ValueError:\n",
    "        data_in[\"tas\"] = data_in.tas * units.kelvin\n",
    "        data_in[\"td\"] = mpcalc.dewpoint_from_specific_humidity(ps, data_in.tas, data_in.huss)\n",
    "\n",
    "    # compute the spatial average\n",
    "    spatial_average = data_in.mean(dim=(\"lat\", \"lon\")).dropna(dim=\"time\")\n",
    "\n",
    "    # sort the file into 6 groups based on soil moisture content\n",
    "    the_max = float(spatial_average.mrsos.max().values)\n",
    "    the_min = float(spatial_average.mrsos.min().values)\n",
    "    the_range =  the_max - the_min\n",
    "\n",
    "    spatial_average[\"soil_moisture_grp\"] = ((spatial_average.mrsos / (the_range / 6)).round() * (the_range / 6)).round()\n",
    "    gbysoil = spatial_average.groupby(spatial_average.soil_moisture_grp)\n",
    "\n",
    "    # remove the highest and lowest values, they are usually too sparse to plot\n",
    "    mrsos_keys = list(gbysoil.groups.keys())\n",
    "    mrsos_keys.remove(max(mrsos_keys))\n",
    "    mrsos_keys.remove(min(mrsos_keys))\n",
    "\n",
    "    # calculate and plot the average diurnal cycle of lcl height\n",
    "    fig, ax = plt.subplots()\n",
    "    lposition = 1.1 # for annotating plot\n",
    "    lines = [\"--\",\"-.\",\":\"]\n",
    "    linecycler = cycle(lines)\n",
    "    for key in mrsos_keys:\n",
    "        # group by hour\n",
    "        hourly_data = gbysoil[key].groupby(gbysoil[key].time.dt.hour).mean(dim=\"time\") \n",
    "\n",
    "        # find and plot the lcl\n",
    "        plcl, tlcl = mpcalc.lcl(ps, hourly_data.tas, hourly_data.td)\n",
    "        plcl_hpa = plcl / 100\n",
    "\n",
    "        # assign colors to match Betts\n",
    "        if key == min(mrsos_keys):\n",
    "            plot_kwargs = {\"color\":\"darkblue\"}\n",
    "        elif key == max(mrsos_keys):\n",
    "            plot_kwargs = {\"color\":\"red\"}\n",
    "        else:\n",
    "            plot_kwargs = {\"color\":\"black\", \"linestyle\":next(linecycler), \"linewidth\":0.8}\n",
    "\n",
    "        # append hour 24 to match hour 0\n",
    "        the_time = np.append(hourly_data.hour.values, 24)\n",
    "        the_lcl = np.append(plcl_hpa, plcl_hpa[0])\n",
    "        ax.plot(the_time, the_lcl, label=f\"{round(key)} kg/m$^3$\", **plot_kwargs)\n",
    "\n",
    "    # make the plot match Betts fig 11\n",
    "    plt.gca().invert_yaxis()\n",
    "    ax.set_xlabel(\"Local Time\")\n",
    "    ax.set_ylabel(\"P$_{LCL}$ (hPa)\")\n",
    "    ax.legend(loc=\"upper center\")\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(6))\n",
    "    ax.xaxis.set_major_formatter('{x:.0f}')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    ax.set_xticks((0,6,12,18,24), labels=(18, 24, 0, 6, 12))\n",
    "    ax.set_xlim(0,24)\n",
    "    ax.set_title(str(nc_data)[5:-9]);\n",
    "    \n",
    "    if save_loc != None:\n",
    "        plt.savefig(f\"{save_loc}/{str(nc_data)[5:-9]}-betts-fig10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "257d9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_betts_fig11(nc_data, save_loc=None):\n",
    "    \"\"\"\n",
    "    Creates a plot matching figure 10 from 'Land-Surface-Atmosphere Coupling in Observations and Models',\n",
    "    by Alan K Betts (2009). Required fields are (interpolated to 3hr or better):\n",
    "    \n",
    "        3hr:  tas        (surface air temp)\n",
    "              huss       (surface humidity)\n",
    "              mrsos      (upper layer soil moisture)\n",
    "              pr         (precipitation rate)\n",
    "              hfls       (surface upward latent heat flux)\n",
    "              hfss       (surface upward sensible heat flux)\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_data : string\n",
    "        path do NETCDF data file saved using download_data() function in this library\n",
    "        \n",
    "    save_loc : string\n",
    "        location to save the figure. default behavior is to render the figure but not save. \n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    None. Saves plot to disk if save_loc is specified\n",
    "    \"\"\"\n",
    "    ps = 100000 * units.Pa\n",
    "    print(str(nc_data)[5:-9])\n",
    "    data_in = xr.open_dataset(nc_data, engine=\"netcdf4\", decode_times=False).metpy.quantify()\n",
    "    data_in[\"time\"] = cftime.num2date(data_in.time, \"hours since 1850-01-01 03:00:00\", calendar=\"noleap\", has_year_zero=True)\n",
    "\n",
    "    # convert to mm/day and assign units to precip rate (units unspecified in raw data, but CMIP6 specs say its reported in kg/m2/s)\n",
    "    # https://docs.google.com/spreadsheets/d/1UUtoz6Ofyjlpx5LdqhKcwHFz2SGoTQV2_yekHyMfL9Y/edit#gid=1221485271\n",
    "    data_in[\"pr\"] = data_in[\"pr\"] * 86400 * units('mm/day')\n",
    "    #data_in[\"pr\"] = data_in.pr * units('mm/day')\n",
    "\n",
    "    # get dew point temp\n",
    "    try:\n",
    "        data_in[\"td\"] = mpcalc.dewpoint_from_specific_humidity(ps, data_in.tas, data_in.huss)\n",
    "    except ValueError:\n",
    "        data_in[\"tas\"] = data_in.tas * units.kelvin\n",
    "        data_in[\"td\"] = mpcalc.dewpoint_from_specific_humidity(ps, data_in.tas, data_in.huss)\n",
    "\n",
    "    # calculate the evaporative fraction (Betts eq 9)\n",
    "    data_in[\"EF\"] = abs(data_in.hfls) / (abs(data_in.hfls) + abs(data_in.hfss))\n",
    "\n",
    "    # compute the spatial average\n",
    "    spatial_average = data_in.mean(dim=(\"lat\", \"lon\")).dropna(dim=\"time\")\n",
    "\n",
    "    # sort the file into 4 groups based on precip rate\n",
    "    the_max = float(spatial_average.pr.max().values)\n",
    "    the_min = float(spatial_average.pr.min().values)\n",
    "    the_range =  the_max - the_min\n",
    "\n",
    "    spatial_average[\"precip_group\"] = ((spatial_average.pr / (the_range / 5)).round() * (the_range / 5)).round()\n",
    "    gbypr = spatial_average.groupby(spatial_average.precip_group)\n",
    "\n",
    "    pr_keys = list(gbypr.groups.keys())\n",
    "    pr_keys.remove(max(pr_keys))\n",
    "    pr_keys.remove(max(pr_keys))\n",
    "\n",
    "    # plot each group\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "    for key in pr_keys:\n",
    "        # find and plot the lcl as a function of soil moisture\n",
    "        plcl, tlcl = mpcalc.lcl(ps, gbypr[key].tas, gbypr[key].td)\n",
    "        plcl_hpa = plcl / 100\n",
    "\n",
    "        alpha=0.5\n",
    "        ax1.scatter(gbypr[key].mrsos, plcl_hpa, label=f\"{round(key)} mm/day\", alpha=alpha)\n",
    "        ax2.scatter(gbypr[key].EF, plcl_hpa, label=f\"{round(key)} mm/day\", alpha=alpha)\n",
    "\n",
    "    ax1.set_xlabel(\"Upper Soil Water Content (kg/m$^3$)\")\n",
    "    ax1.set_ylabel(\"P$_{LCL}$ (hPa)\")\n",
    "    ax1.invert_yaxis()\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "    ax2.set_xlabel(\"Evaporative Fraction\")\n",
    "\n",
    "    fig.suptitle(f\"{str(nc_data)[5:-9]}\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_loc != None:\n",
    "        plt.savefig(f\"{save_loc}/{str(nc_data)[5:-9]}-betts-fig11\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
